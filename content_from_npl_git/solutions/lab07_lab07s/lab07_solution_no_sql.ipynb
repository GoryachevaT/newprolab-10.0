{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение Лаб07 и  суперачивки на датафреймах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json('/labs/lab07data/DO_record_per_line.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'desc', 'id', 'lang', 'name', 'provider']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cat: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- provider: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Курсы,  по которым нужно выдать решение\n",
    "given = [\n",
    "    [8150, u'en', u'StatLearning: Statistical Learning'], \n",
    "    [25679, u'en', u'Video Lighting Basics - Udemy'], \n",
    "    [7791, u'es', u'Programaci\\xf3n CNC - Fresadoras'], \n",
    "    [23111, u'es', u'C\\xf3mo Crear un Blog Gratis en Google Blogger - Udemy'], \n",
    "    [1396, u'ru', u'\\u0412\\u0432\\u0435\\u0434\\u0435\\u043d\\u0438\\u0435 \\u0432\\u043e \\u0432\\u0441\\u0442\\u0440\\u043e\\u0435\\u043d\\u043d\\u044b\\u0435 \\u0441\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b \\u0438 Windows Embedded CE'], \n",
    "    [1348, u'ru', u'\\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u044f Microsoft ADO .NET']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_langs = [ (a[0],a[1]) for a in given]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF + TFIDF + dot product + l2_norm (via Spark's inner funcs, FAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация, как в задании\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация, как в задании\n",
    "import pyspark.sql.functions as f\n",
    "import re\n",
    "\n",
    "@f.udf(ArrayType(StringType()))\n",
    "def re_tokenizer(text):\n",
    "    regex = re.compile(r'[\\w\\d]{2,}', re.U)\n",
    "    return regex.findall(text.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsData_udf = data.withColumn('words', re_tokenizer('desc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? Токенизация 2: Плохо парсит русскую кодировку \n",
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Term Frequencies using HashingTF function\n",
    "hashingTF = HashingTF(inputCol=\"words\",\n",
    "                      outputCol=\"TFFeatures\",\n",
    "                      numFeatures=10000, )\n",
    "featurizedData = hashingTF.transform(wordsData_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculationg Inverse Document Frequencies\n",
    "idf = IDF(inputCol=\"TFFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация векторов L2, после этого для Cosine_similarity будет достаточно \n",
    "# делать dot product нормализованных векторов\n",
    "from pyspark.ml.feature import Normalizer\n",
    "t = Normalizer(inputCol='features', outputCol='norm_features', p=2.0)\n",
    "normalizedData = t.transform(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:42<00:00,  7.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# For each given course caclulate cosine similarity to any other\n",
    "# chose top 10\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = normalizedData\n",
    "dict_out = {}\n",
    "\n",
    "for course_id, lang in tqdm(courses_langs):\n",
    "    #get l2-normalized SV for current text\n",
    "    vec1 = df.filter(df.id == int(course_id))\\\n",
    "             .collect()[0]['norm_features'].toArray()\n",
    "    \n",
    "    #we need to define it as lambda, so that it takes vec1 with it\n",
    "    #If you declare it as a function with vec1 as a global var insde it, it won't work\n",
    "    #I also could not manage to use broadcasts\n",
    "    # and culd not manage to create a \"constant\" column of with vec1 in all rows\n",
    "    dp = f.udf(lambda x: float(x.dot(vec1)), FloatType())\n",
    "    \n",
    "    #where('id <> {0} and lang = \\'{1}\\''.format(str(course_id), lang))\\\n",
    "    df_sim = df.where((df.id != int(course_id)) & (df.lang == lang))\\\n",
    "               .withColumn('cosine_sim', dp(df['norm_features']))\\\n",
    "               .orderBy(f.desc('cosine_sim'), f.asc('name') ,f.asc('id'))\\\n",
    "               .head(10)\n",
    "                           \n",
    "    list_out = [x['id'] for x in df_sim]\n",
    "    dict_out.update({str(course_id): list_out})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'8150': [13273, 8145, 16837, 8146, 26907, 22411, 8306, 8142, 540, 542],\n",
       " '25679': [7297, 4466, 24891, 5019, 4799, 4290, 8588, 6243, 6129, 15232],\n",
       " '7791': [21853, 10738, 10035, 21107, 11474, 387, 386, 22051, 19153, 4096],\n",
       " '23111': [9285, 13224, 9352, 6864, 26336, 26670, 9286, 387, 19404, 10668],\n",
       " '1396': [1006, 20314, 8215, 1235, 1347, 20102, 994, 934, 12202, 890],\n",
       " '1348': [1257, 823, 819, 20307, 829, 1285, 1256, 20292, 1229, 810]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict_out # Верный вариант 8\n",
    "{'1348': [1257, 823, 819, 20307, 829, 1285, 1256, 20292, 1229, 810],\n",
    " '1396': [1006, 20314, 8215, 1235, 1347, 20102, 994, 934, 12202, 890],\n",
    " '23111': [9285, 13224, 9352, 6864, 26336, 26670, 9286, 387, 19404, 10668],\n",
    " '25679': [7297, 4466, 24891, 5019, 4799, 4290, 8588, 6243, 6129, 15232],\n",
    " '7791': [21853, 10738, 10035, 21107, 11474, 387, 386, 22051, 19153, 4096],\n",
    " '8150': [13273, 8145, 16837, 8146, 26907, 22411, 8306, 8142, 540, 542]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('lab07s.json', 'w') as fout:\n",
    "    fout.write(json.dumps(dict_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

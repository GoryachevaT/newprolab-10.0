## ✨ Лаба 2. Суперачивка. Построить TOP350 самых популярных  URL из большого лог-файла, лежащего в HDFS

##### [![New Professions Lab — Big Data 9](http://data.cluster-lab.com/public-newprolab-com/npl7.svg)](https://github.com/newprolab/content_bigdata9)

### Дедлайн

⏰ Вторник, 9 апреля 2019 года, 23:59 по Москве.

### Задача

Вы планируете рекламную кампанию, и из имеющихся логов посещения пользователей вы хотите выделить 350 наиболее популярных веб-сайтов, которые войдут в лонг-лист потенциальных площадок для размещения баннеров.

#### Структура данных

Как и в [лабораторной работе 2](labs/lab02/lab02.md), вы будете работать с датасетом facetz, с которым вы уже познакомились. Однако теперь вам нужно взять другую директорию: `facetz_2015_02_12`. 

#### Обработка данных на вход

Для выполнения работы вам следует взять файлы из директории `facetz_2015_02_12`.

⚠️ Замечание: строки от анонимных пользователей (UID = `'-'`) и с пустым URL следует не учитывать при расчете TOP350.  URL не нужно нормализовывать.

#### Обработка данных на выход

Выходной файл должен быть расположен **не в HDFS**, а в вашей домашней директории на gateway ноде кластера, к которой вы обычно подключаетесь по ssh, и называться `top350.txt`.

Формат файла: URL, знак табуляции, число обращений.

#### Подсказка

Если вы используете Hadoop Streaming, то для получения TOP350 по результатам, полученным из Hadoop, можно запустить второй MR job, но уже с единственным Reducer-ом. Или можно воспользоваться unix-командами `sort` и `head`.

#### Проверка

Проверка осуществляется автоматическим скриптом из Личного кабинета. 
